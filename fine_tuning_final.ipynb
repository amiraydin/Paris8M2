{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx8s7tZ1jiS0"
      },
      "source": [
        "# **PROJET FINE-TUNING LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tXe5c7ijiS5"
      },
      "source": [
        "## ***Importer les librairies usuelles*** ##"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "!pip install trl\n",
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7lCl6Ypm4jl",
        "outputId": "4637bad5-1b6c-4a0b-8674-df1b925d38ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-edn9496x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-edn9496x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dENoJ5nnjiS6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "from numba import cuda\n",
        "from transformers import GPT2LMHeadModel, GPTNeoForCausalLM, GPT2Tokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NR_IF9YjiTb"
      },
      "source": [
        "## ***Importer le datset*** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfEOeqS1jiTd"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"manu/french_poetry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMYWyFfojiTf"
      },
      "outputs": [],
      "source": [
        "# Extraire le Datset grâce à la clé \"train\"\n",
        "df = dataset['train'].to_pandas()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xnMFbZkjiTu"
      },
      "source": [
        "## ***Explorer le Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOBIrq_AjiT3"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_4C1L6zjiT8"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TseiPDlnjiT_"
      },
      "outputs": [],
      "source": [
        "df['text'].iloc[50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5yK4soXjiUA"
      },
      "source": [
        "## ***Effectuer le preprocessing sur le dataset*** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jixMJlHQjiUC"
      },
      "outputs": [],
      "source": [
        "# Récupérer la section poème positionnée à l'indice 4\n",
        "df['text'] = df['text'].str.split(':').str[4]\n",
        "\n",
        "# Garder la section poème positionnée après le premier \"\\n\"\n",
        "df['text'] = df['text'].str.split('\\n', n=1).str[1]\n",
        "\n",
        "# Supprimer l'intégralité des NAN (poèmes structure non conforme)\n",
        "#df['text'] = df['text'].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvft1IrJjiUD"
      },
      "outputs": [],
      "source": [
        "df.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][0]"
      ],
      "metadata": {
        "id": "v2PoRWZ9lAnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-90bt1_HjiUD"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtpYQJ8kjiUE"
      },
      "source": [
        "## ***Suppression des valeurs NaN aprés transformation ne respectant pas la structure globale***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xs1wliAjiUN"
      },
      "outputs": [],
      "source": [
        "# Détecter les valeurs NaN\n",
        "missing_values = df.text.isnull().sum()\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Krf3hltjiUR"
      },
      "outputs": [],
      "source": [
        "# Supprimer les valeurs Nan\n",
        "df = df.dropna(subset=['text']).reset_index(drop =True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtLMlf-ijiUX"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_wmu4bDjiUb"
      },
      "outputs": [],
      "source": [
        "# Compter les valeurs NaN après suppression\n",
        "missing_values = df.text.isnull().sum()\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i37EIEECjiUd"
      },
      "outputs": [],
      "source": [
        "df['title'].iloc[4]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'].iloc[4]"
      ],
      "metadata": {
        "id": "Ol4bLqpnv0Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gph9OLj2jiUe"
      },
      "outputs": [],
      "source": [
        "# Suppression des colonnes inutiles\n",
        "df = df.drop(columns = ['poet', 'link', 'id'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "D9KsXod1laXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgxBkrwKjiUf"
      },
      "outputs": [],
      "source": [
        "# Dictionnaire des prompts utilisés pour améliorer la robustesse du dataset\n",
        "\"\"\"prompt_variance = [\n",
        "    \"Écris un poème sur le thème : {title}\",\n",
        "    \"Compose un poème intitulé : {title}\",\n",
        "    \"Imagine un poème ayant pour sujet : {title}\",\n",
        "    \"Crée un poème autour de : {title}\",\n",
        "    \"Un poème inspiré de {title}, s'il te plaît\",\n",
        "    \"{title}\",\n",
        "    \"Rédige un poème poétique à propos de : {title}\",\n",
        "    \"Propose un poème sur le sujet suivant : {title}\",\n",
        "    \"Donne-moi un poème avec pour thème principal : {title}\",\n",
        "    \"Développe un poème basé sur : {title}\",\n",
        "    \"Invente un poème portant sur : {title}\",\n",
        "    \"Un poème intitulé {title}\",\n",
        "    \"Sur le thème {title}, écris un poème captivant\",\n",
        "    \"Conçois un poème ayant comme sujet : {title}\",\n",
        "    \"Pour {title}, écris un poème unique\",\n",
        "    \"À partir du sujet '{title}', rédige un poème\",\n",
        "    \"Avec {title}, inspire-toi pour créer un poème\",\n",
        "    \"Un poème autour de {title}, si possible\",\n",
        "    \"Donne-moi un poème avec pour thème {title}\",\n",
        "    \"Fais un poème en explorant le thème {title}\",\n",
        "    \"Compose une œuvre poétique sur {title}\",\n",
        "    \"Rédige un poème à propos de {title}\"\n",
        "]\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsyGxt51jiUg"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Fixer une seed pour la reproductibilité des résultats\n",
        "random.seed(42)\n",
        "\n",
        "# Applqiuer les différents prompt aléatoirement sur les titres des poèmes\n",
        "df['prompt'] = df['title'].apply(lambda title: random.choice(prompt_variance).format(title=title))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GdX2oUBjiUh"
      },
      "outputs": [],
      "source": [
        "# Conserver uniquement les colonnes prompt et text\n",
        "#df = df[['prompt', 'text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF0uWRcxjiUo"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIZ5hQibjiVB"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s1jRrQhjiVC"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yira75v3jiVD"
      },
      "source": [
        "Tokens spéciaux : {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPVX_Z6WjiVG"
      },
      "source": [
        "## ***Ajouter les token spéciaux au dataset qui ne sont pas présent par déafut dans le tokenizer*** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq-M1SpdjiVI"
      },
      "outputs": [],
      "source": [
        "# token saut de ligne\n",
        "#df['text'] = df[\"text\"].str.replace(\"\\n\", \"<|newline|>\", regex = False)\n",
        "#df['text'] = df[\"text\"].str.replace(\"\\n\", \"<|newline|>\", regex = False)\n",
        "\n",
        "# token padding\n",
        "# Ajouter des tokens <|pad|> pour chaque ligne\n",
        "#target_length = 1489\n",
        "\n",
        "\"\"\"df[\"padded_text\"] = df[\"text\"] + df.apply(\n",
        "    lambda row: \" \" + \" \".join([\"<|pad|>\"] * (target_length - row[\"text_length\"]))\n",
        "    if row[\"text_length\"] < target_length else \"\", axis=1\n",
        ")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KqXyzqpjiVK"
      },
      "outputs": [],
      "source": [
        "# Conserver uniquement les colonnes prompt et text\n",
        "#df = df[['prompt', 'text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffTILMGojiVO"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXsU1on6jiVR"
      },
      "outputs": [],
      "source": [
        "df.text[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUfy2M19jiVT"
      },
      "outputs": [],
      "source": [
        "# Convertir le dataset en CSV\n",
        "df.to_csv('poems_dataset.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6b4J9SKjiVW"
      },
      "source": [
        "## ***Importer le modèle pour le tester sans fine-tuning*** ##"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('poems_prompts_dataset.csv')"
      ],
      "metadata": {
        "id": "tvpSy6252iaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tefo3icbjiVc"
      },
      "outputs": [],
      "source": [
        "\"\"\"from transformers import pipeline\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "generator(\"EleutherAI has\", do_sample=True, min_length=50)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w_OoN71jiVh"
      },
      "outputs": [],
      "source": [
        "\"\"\"prompt = \"Génère un poème sur l'amitié\"\n",
        "res = generator(prompt, max_length = 200, top_k = 80, top_p = 0.90, do_sample = True, temperature = 1.1)\n",
        "\n",
        "print(res[0]['generated_text'])\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM7SzTbPjiVh"
      },
      "outputs": [],
      "source": [
        "#generator(\"Génère un poème sur le thème du courage\", do_sample = True, max_length = 200, min_length = 150, temperature = 1, top_k = 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpWYkCCWjiVi"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtrZ7KspjiVo"
      },
      "outputs": [],
      "source": [
        "df.text[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgLsapXNjiVs"
      },
      "source": [
        "## ***Begining of NLP*** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Imports libs here***#"
      ],
      "metadata": {
        "id": "-bPpAxxFzoy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, default_data_collator\n",
        "from transformers import DataCollatorForLanguageModeling, default_data_collator\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "ULfDwxexSsH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apprentissage masqué (Masked Language Modeling, MLM) pour des modèles comme BERT.\n",
        "# mlm : Un booléen qui indique si le masquage pour MLM doit être appliqué (par défaut, True).\n",
        "\n",
        "\"\"\"def load_data_collator(tokenizer, mlm = False):\n",
        "  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=mlm, )\n",
        "  return data_collator\"\"\""
      ],
      "metadata": {
        "id": "juQ10T6ky3s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['combined'] = '[Q]' + df['title'] + ' ' + '[A]' + df['text']"
      ],
      "metadata": {
        "id": "3Q9Bikxay3cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "G3MDnc5g5JPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Fractionner en train/test\n",
        "train_data, test_data = train_test_split(df['combined'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "JIRb-Jev6S78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir le dataset en datframe hugging face\n",
        "from datasets import Dataset, DatasetDict"
      ],
      "metadata": {
        "id": "Qkv5NUpY8u9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir en Dataset Hugging Face\n",
        "train_dataset = Dataset.from_dict({\"text\": train_data})\n",
        "test_dataset = Dataset.from_dict({\"text\": test_data})\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
      ],
      "metadata": {
        "id": "wH10Lx076SxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJfB5NgbjiVt"
      },
      "outputs": [],
      "source": [
        "#from transformers import AutoTokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Afficher les token par défaut\n",
        "# Afficher les tokens spéciaux par défaut\n",
        "print(\"Tokens spéciaux par défaut :\", tokenizer.special_tokens_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx1EiXY7jiVu"
      },
      "outputs": [],
      "source": [
        "#from transformers import AutoModelForCausalLM\n",
        "# Ajouter un nouveau token spécial\n",
        "#tokenizer.add_special_tokens({\"unk_token\": \"<|unknown|>\"})\n",
        "#tokenizer.add_special_tokens({\"pad_token\": \"<|unknown|>\"})\n",
        "\n",
        "# Charger le modèle\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\n",
        "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
        "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))"
      ],
      "metadata": {
        "id": "9hO1-bJ7R6W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "7vTv9-snSNNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "1iIDzoj8SQsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.all_special_tokens"
      ],
      "metadata": {
        "id": "SgSAqNGeSUFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "7f5soFVySZpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.max_model_input_sizes"
      ],
      "metadata": {
        "id": "lbER3N3iSdtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"sentence = 'I am a Data Science and Artificial Inteligence Dev who know to train a model'\n",
        "input_ids  = tokenizer.encode(sentence, return_tensors = 'pt')\"\"\""
      ],
      "metadata": {
        "id": "JV9fkW3DS43M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_ids"
      ],
      "metadata": {
        "id": "pwhgJKrSS4yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.decode(input_ids[0][7])"
      ],
      "metadata": {
        "id": "wgEzOntxTBYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gSKFFp4jiVw"
      },
      "outputs": [],
      "source": [
        "print(\"Taille du vocabulaire du tokenizer :\", len(tokenizer))\n",
        "print(\"Taille du vocabulaire du modèle :\", model.config.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = './results'\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "T27BPYRNSdo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ_4LXWjiVw"
      },
      "source": [
        "## ***Ajouter les tokens préalablement créés au tokenizer***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHdcniFOjiVy"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNWhfweMjiVz"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psxdlRSgjiV0"
      },
      "outputs": [],
      "source": [
        "#df['combined'] = '[Q]' + df['prompt'] + ' ' + '[A]' + df['text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df['combined'] = re.sub(r'\\n+', '\\n', text_data).strip()  # Remove excess newline characters"
      ],
      "metadata": {
        "id": "2UWPkTRHxNfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNjheBIvjiV7"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdxXKx7hjiV8"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKBC9WIUjiV9"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier et définir un token de padding\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Utiliser eos_token comme pad_token\n",
        "    # Ou ajouter un nouveau token spécial\n",
        "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    # model.resize_token_embeddings(len(tokenizer))  # Adapter les embeddings"
      ],
      "metadata": {
        "id": "FWrCsybLHQYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de tokenisation pour vérifier\n",
        "texts = [\"Hello world!\", \"GPT-2 fine-tuning\"]\n",
        "encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(encodings)"
      ],
      "metadata": {
        "id": "xTXePvgTHZ8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajouter le DataCollator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Mettez True si vous faites du Masked Language Modeling (par ex., avec BERT)\n",
        ")"
      ],
      "metadata": {
        "id": "omucuqk0C1Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisation\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=False)  # Pas de padding ici, géré par le DataCollator\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "qnSujvO4BWR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJCVQEEzjiWB"
      },
      "source": [
        "emplacement fichier config : accelerate configuration saved at C:\\Users\\33760/.cache\\huggingface\\accelerate\\default_config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "8f7_G4-WFL2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets['train']"
      ],
      "metadata": {
        "id": "E3uvQ-QKFW8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt-neo-125M-fine-tuned-poetry\",             # Répertoire pour sauvegarder les résultats et checkpoints\n",
        "    eval_strategy=\"epoch\",       # Évaluer le modèle à la fin de chaque époque\n",
        "    learning_rate=5e-5,                # Taux d'apprentissage (learning rate)\n",
        "    per_device_train_batch_size=1,     # Taille du batch par appareil (GPU ou CPU)\n",
        "    num_train_epochs=1,                # Nombre total d'époques d'entraînement\n",
        "    save_steps=500,                    # Sauvegarder le modèle toutes les 500 étapes\n",
        "    save_total_limit=2,                # Conserver uniquement les 2 dernières sauvegardes\n",
        "    fp16=False,                        # Activer l'entraînement en virgule flottante 16 bits (plus rapide si GPU compatible)\n",
        "    logging_dir = \"./logs\",\n",
        ")\"\"\""
      ],
      "metadata": {
        "id": "5qDnLfVVqwVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgIfs_ZPjiWD"
      },
      "outputs": [],
      "source": [
        "# Importer les modules nécessaires de la bibliothèque transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Configurer les arguments d'entraînement pour le fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/trained/\",\n",
        "    overwrite_output_dir=False,\n",
        "    #debug=\"underflow_overflow\",\n",
        "    #eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=5,\n",
        "    #per_device_eval_batch_size=1,\n",
        "    num_train_epochs=50,\n",
        "    save_steps=5000,\n",
        "    #save_total_limit=1,\n",
        "    fp16=True,  # Utiliser la précision mixte si disponible\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=20,\n",
        ")\n",
        "\n",
        "# Configurer le Trainer, une classe de Hugging Face pour gérer l'entraînement\n",
        "trainer = Trainer(\n",
        "    model=model,                       # Modèle à fine-tuner\n",
        "    args=training_args,                # Arguments d'entraînement configurés précédemment\n",
        "    train_dataset=tokenized_datasets[\"train\"],    # Jeu de données tokenisé utilisé pour l'entraînement\n",
        "    #eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Lancer le processus d'entraînement\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jViX0j1vjiWD"
      },
      "outputs": [],
      "source": [
        "# Sauvegarder le modèle fine-tuné dans le répertoire : ./gpt-neo-125M-fine-tuned-poetry\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder le modèle fine-tuné\n",
        "model.save_pretrained(\"./gpt2-poetry\")\n",
        "tokenizer.save_pretrained(\"./gpt2-poetry\")"
      ],
      "metadata": {
        "id": "qXm_6tbwA5YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generator = pipeline('text-generation', model='./gpt2-poetry', tokenizer='./gpt2-poetry')\n",
        "#print(generator(\"Compose un poème sur le bonheur\", max_length=200, temperature=0.8))\n"
      ],
      "metadata": {
        "id": "VDVcpoMRBeC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
        "\n",
        "def load_model(model_path):\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "  return model\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "  return tokenizer\n",
        "\n",
        "def generate_text(model_path, sequence, max_length):\n",
        "  model = load_model(model_path)\n",
        "  tokenizer = load_tokenizer(model_path)\n",
        "  ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "  final_outputs = model.generate(\n",
        "      ids,\n",
        "      do_sample=True,\n",
        "      max_length=max_length,\n",
        "      pad_token_id=model.config.eos_token_id,\n",
        "      top_k=50,\n",
        "      top_p=0.95,\n",
        "  )\n",
        "  print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "uiJBCO9fI8V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2_path = \"./gpt2-poetry\"\n",
        "sequence2 = \"[Q] un poème sur Nouvel amour \"\n",
        "max_len = 250\n",
        "generate_text(model2_path, sequence2, max_len)"
      ],
      "metadata": {
        "id": "ZtEv8vOTK4Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEWWEf2tjiWD"
      },
      "outputs": [],
      "source": [
        "#tokenizer.save_pretrained(\"./gpt-neo-125M-fine-tuned-poetry/tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMFkOIDbjiWE"
      },
      "outputs": [],
      "source": [
        "\"\"\"def calculate_rouge(reference, candidate):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, candidate)\n",
        "    return scores['rouge1'].fmeasure\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxe2SFpGjiWE"
      },
      "outputs": [],
      "source": [
        "\"\"\"def generate_poem(model, tokenizer, keyword, max_length=100):\n",
        "    if not keyword.strip():\n",
        "        raise ValueError(\"Le mot-clé ne peut pas être vide. Veuillez entrer un mot valide.\")\n",
        "\n",
        "    input_ids = tokenizer.encode(keyword, return_tensors=\"pt\")\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    generated_poem = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_poem\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHmGn0zRjiWE"
      },
      "outputs": [],
      "source": [
        "\"\"\"def calculate_bleu(reference, candidate):\n",
        "    return sentence_bleu([reference], candidate, smoothing_function=SmoothingFunction().method1)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8zBGcVTjiWF"
      },
      "outputs": [],
      "source": [
        "\"\"\"from transformers import GPT2LMHeadModel, GPTNeoForCausalLM, GPT2Tokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\"\"\"\n",
        "#from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PKj2ojEjiWF"
      },
      "outputs": [],
      "source": [
        "\"\"\"gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "gptneo_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "gptneo_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./gpt-neo-125M-fine-tuned-poetry/tokenizer\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt-neo-125M-fine-tuned-poetry\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w9tS5_tjiWG"
      },
      "outputs": [],
      "source": [
        "\"\"\"keyword = input(\"Entrez un mot-clé pour générer un poème : \").strip()\n",
        "\n",
        "print(keyword)\n",
        "\n",
        "if len(keyword) == 0:\n",
        "    print(\"Erreur : Le mot-clé ne peut pas être vide.\")\n",
        "    exit()\n",
        "\n",
        "if len(keyword) > 50:\n",
        "    print(\"Erreur : Le mot-clé est trop long. Veuillez entrer un mot ou une courte phrase.\")\n",
        "    exit()\n",
        "\n",
        "# Générer des poèmes pour chaque modèle\n",
        "gpt2_poem = generate_poem(gpt2_model, gpt2_tokenizer, keyword)\n",
        "#gptneo_poem = generate_poem(gptneo_model, gptneo_tokenizer, keyword)\n",
        "model_poem = generate_poem(model, gptneo_tokenizer, keyword)\n",
        "\n",
        "# Calculer les scores BLEU et ROUGE\n",
        "bleu_score = calculate_bleu(gpt2_poem, model_poem)\n",
        "rouge_score = calculate_rouge(gpt2_poem, model_poem)\n",
        "\n",
        "# Afficher les poèmes générés et les scores\n",
        "print(\"\\nPoème généré par GPT-2 :\\n\", gpt2_poem)\n",
        "#print(\"\\nPoème généré par GPT-Neo :\\n\", gptneo_poem)\n",
        "print(\"\\nPoème généré par model :\\n\", model_poem)\n",
        "print(\"\\nScore BLEU entre les poèmes générés : \", bleu_score)\n",
        "print(\"Score ROUGE entre les poèmes générés : \", rouge_score)\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}